{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "## We shall use a sample text to create our preprocessing functions\n",
    "\n",
    "### Before that, lets perform some string manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A string is defined within quotes (single or double)\n",
    "#join() method returns a string in which string elements of sequence (B) have been joined by 'A' Seperator.\n",
    "A = 'Insofe'\n",
    "B = 'Batch48'\n",
    "C = 'X123'\n",
    "D = '123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BInsofeaInsofetInsofecInsofehInsofe4Insofe8\n",
      "B a t c h 4 8\n",
      "InsofeBatch48\n"
     ]
    }
   ],
   "source": [
    "print(A.join(B))\n",
    "print(\" \".join(B))\n",
    "print(A+B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insofe\n",
      "INSOFE\n",
      "-1\n",
      "0\n",
      "BInsofeaInsofetInsofecInsofehInsofe4Insofe8\n",
      "B a t c h 4 8\n",
      "InsofeBatch48\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Lets perform some string manipulations on the string\n",
    "print (A.lower())\n",
    "print(A.upper())\n",
    "print(A.find('a'))\n",
    "print(A.count('a'))\n",
    "print(A.join(B))\n",
    "print(' '.join(B))\n",
    "print(A+B)\n",
    "print(A.isdigit())\n",
    "print(D.isdigit())\n",
    "print(C.isdigit())\n",
    "print(C.isalnum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsofe\n",
      "Insofe\n",
      "Insofe\n",
      "Isf\n",
      "efosnI\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "print(A.strip('I')) #Python String strip() method returns a copy of the string with both leading and trailing characters removed\n",
    "print(A)\n",
    "print(A[::1])  #Seq[Start:End:Step]\n",
    "print(A[::2])  #Seq[Start:End:Step]\n",
    "print(A[::-1]) #Seq[Start:End:Step] Reverse\n",
    "print(A[0])\n",
    "#A[0]='shr'     #Strings are immutable    ---- 'str' object does not support item assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''\n",
    "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'Leatherhead,', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes.', 'It', 'was', 'a', 'perfect', 'day,', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots,', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap,', 'his', 'arms', 'folded,', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes,', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast,', 'buried', 'in', 'the', 'deepest', 'thought.', 'Suddenly,', 'however,', 'he', 'started,', 'tapped', 'me', 'on', 'the', 'shoulder,', 'and', 'pointed', 'over', 'the', 'meadows.']\n"
     ]
    }
   ],
   "source": [
    "##Some string manipulations- Splitting the paragraph by spaces\n",
    "Str_list = string.split() # split() method returns a list of strings after breaking the given string by the specified separator.\n",
    "print(Str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nAt Waterloo we were fortunate in catching a don't train for Leatherhead\", ' where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day', ' with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots', ' and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion sat in the front of the trap', ' his arms folded', ' his hat pulled down over his eyes', ' and his chin sunk upon his breast', ' buried in the deepest thought. \\nSuddenly', ' however', ' he started', ' tapped me on the shoulder', ' and pointed over the meadows.\\n']\n"
     ]
    }
   ],
   "source": [
    "## Splitting the paragraph by a punctuation marks\n",
    "Str_list = string.split(\",\") #split() method returns a list of strings after breaking the given string by the specified separator.\n",
    "print(Str_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But what if we want to read a text file?\n",
    "#### We first check the working directory using the \"os\" module and the \".getcwd()\" method in it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # The OS module in Python provides a way of using operating system dependent functionality. \n",
    "# Return information identifying the current operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And then set it to required path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Studies\\INSOFE\\Practicals\\Lab Activity\\Module-7 - Text Mining and Search\\Day-1 - Text_Mining\\20181124_Batch48_CSE7124c_TextMiningIntroduction\\20181124_Batch48_CSE7124c_TextMiningIntroduction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Studies\\\\INSOFE\\\\Practicals\\\\Lab Activity\\\\Module-7 - Text Mining and Search\\\\Day-1 - Text_Mining\\\\20181124_Batch48_CSE7124c_TextMiningIntroduction\\\\20181124_Batch48_CSE7124c_TextMiningIntroduction'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "print(path)\n",
    "os.chdir(\"C:/Studies/INSOFE/Practicals/Lab Activity/Module-7 - Text Mining and Search/Day-1 - Text_Mining/20181124_Batch48_CSE7124c_TextMiningIntroduction/20181124_Batch48_CSE7124c_TextMiningIntroduction\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we could even verify what files are there in the path as shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '20181124_Batch48_CSE7124c_TextPreprocessing_Sol.ipynb', '20181124_Batch48_CSE7124c_TFIDF.ipynb', '20181124_Batch48_CSE7124c_TfIdfManualCalculation.xlsx', 'english_german_articles.txt', 'sherlock.txt']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we have our required text file(s)/folder(s) in the working directory we use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sherlock.txt', 'r') as x:\n",
    "    string1 = x.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \\nIt was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \\nThe trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \\nMy companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \\nSuddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'r' in the code stands for read operation. One would use 'w' to write to a file and 'a' to append to an existing file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now that our sample text is ready, let us perform the following steps\n",
    "1. Sentence Tokenizing\n",
    "2. Word Tokenizing\n",
    "3. Stop Word Removal\n",
    "4. Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"\\nAt Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\", 'It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.', 'The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.', 'To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.', 'My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.', 'Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes.\n",
      "\n",
      "\n",
      "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens.\n",
      "\n",
      "\n",
      "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth.\n",
      "\n",
      "\n",
      "To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged.\n",
      "\n",
      "\n",
      "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought.\n",
      "\n",
      "\n",
      "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    print(sent)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', 'do', \"n't\", 'train', 'for', 'Leatherhead', ',', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', '.', 'It', 'was', 'a', 'perfect', 'day', ',', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', '.', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', ',', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', '.', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', '.', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', ',', 'his', 'arms', 'folded', ',', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', ',', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', ',', 'buried', 'in', 'the', 'deepest', 'thought', '.', 'Suddenly', ',', 'however', ',', 'he', 'started', ',', 'tapped', 'me', 'on', 'the', 'shoulder', ',', 'and', 'pointed', 'over', 'the', 'meadows', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the above list notice how \"don't\" has been tokenized to 'do' and \"n't\" in word tokens in the above case. This wouldn't have been the case if we did a ```string.split()``` based on spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>[ Character set. Match any character in the set/\n",
    "<br>\t\\w Word. Matches any word character (alphanumeric & underscore).\n",
    "<br>\t' Character. Matches a ' Character\n",
    "<br>] \n",
    "<br>\n",
    "<br>+ Quantifier. Match 1 or more of the preceding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'Leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'Surrey', 'lanes', 'It', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'The', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'To', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'My', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'Suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "####Using regular expressions\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case lowering\n",
    "```python\n",
    "lower_tokens = [] #empty list\n",
    "for token in tokens:\n",
    "    lower_tokens.append(token.lower())\n",
    "```\n",
    "#### But here's a nice use of list comprehensions. Instead of initiating an empty list, writing a for loop and keep appending lower case words, we can save much space by doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'the', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'to', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'my', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "print(lower_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal\n",
    "stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total English Stopwords:  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      " Total German Stopwords:  231\n",
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n",
      "\n",
      " Total Greek Stopwords:  265\n",
      "['αλλα', 'αν', 'αντι', 'απο', 'αυτα', 'αυτεσ', 'αυτη', 'αυτο', 'αυτοι', 'αυτοσ', 'αυτουσ', 'αυτων', 'αἱ', 'αἳ', 'αἵ', 'αὐτόσ', 'αὐτὸς', 'αὖ', 'γάρ', 'γα', 'γα^', 'γε', 'για', 'γοῦν', 'γὰρ', \"δ'\", 'δέ', 'δή', 'δαί', 'δαίσ', 'δαὶ', 'δαὶς', 'δε', 'δεν', \"δι'\", 'διά', 'διὰ', 'δὲ', 'δὴ', 'δ’', 'εαν', 'ειμαι', 'ειμαστε', 'ειναι', 'εισαι', 'ειστε', 'εκεινα', 'εκεινεσ', 'εκεινη', 'εκεινο', 'εκεινοι', 'εκεινοσ', 'εκεινουσ', 'εκεινων', 'ενω', 'επ', 'επι', 'εἰ', 'εἰμί', 'εἰμὶ', 'εἰς', 'εἰσ', 'εἴ', 'εἴμι', 'εἴτε', 'η', 'θα', 'ισωσ', 'κ', 'καί', 'καίτοι', 'καθ', 'και', 'κατ', 'κατά', 'κατα', 'κατὰ', 'καὶ', 'κι', 'κἀν', 'κἂν', 'μέν', 'μή', 'μήτε', 'μα', 'με', 'μεθ', 'μετ', 'μετά', 'μετα', 'μετὰ', 'μη', 'μην', 'μἐν', 'μὲν', 'μὴ', 'μὴν', 'να', 'ο', 'οι', 'ομωσ', 'οπωσ', 'οσο', 'οτι', 'οἱ', 'οἳ', 'οἷς', 'οὐ', 'οὐδ', 'οὐδέ', 'οὐδείσ', 'οὐδεὶς', 'οὐδὲ', 'οὐδὲν', 'οὐκ', 'οὐχ', 'οὐχὶ', 'οὓς', 'οὔτε', 'οὕτω', 'οὕτως', 'οὕτωσ', 'οὖν', 'οὗ', 'οὗτος', 'οὗτοσ', 'παρ', 'παρά', 'παρα', 'παρὰ', 'περί', 'περὶ', 'ποια', 'ποιεσ', 'ποιο', 'ποιοι', 'ποιοσ', 'ποιουσ', 'ποιων', 'ποτε', 'που', 'ποῦ', 'προ', 'προσ', 'πρόσ', 'πρὸ', 'πρὸς', 'πως', 'πωσ', 'σε', 'στη', 'στην', 'στο', 'στον', 'σόσ', 'σύ', 'σύν', 'σὸς', 'σὺ', 'σὺν', 'τά', 'τήν', 'τί', 'τίς', 'τίσ', 'τα', 'ταῖς', 'τε', 'την', 'τησ', 'τι', 'τινα', 'τις', 'τισ', 'το', 'τοί', 'τοι', 'τοιοῦτος', 'τοιοῦτοσ', 'τον', 'τοτε', 'του', 'τούσ', 'τοὺς', 'τοῖς', 'τοῦ', 'των', 'τό', 'τόν', 'τότε', 'τὰ', 'τὰς', 'τὴν', 'τὸ', 'τὸν', 'τῆς', 'τῆσ', 'τῇ', 'τῶν', 'τῷ', 'ωσ', \"ἀλλ'\", 'ἀλλά', 'ἀλλὰ', 'ἀλλ’', 'ἀπ', 'ἀπό', 'ἀπὸ', 'ἀφ', 'ἂν', 'ἃ', 'ἄλλος', 'ἄλλοσ', 'ἄν', 'ἄρα', 'ἅμα', 'ἐάν', 'ἐγώ', 'ἐγὼ', 'ἐκ', 'ἐμόσ', 'ἐμὸς', 'ἐν', 'ἐξ', 'ἐπί', 'ἐπεὶ', 'ἐπὶ', 'ἐστι', 'ἐφ', 'ἐὰν', 'ἑαυτοῦ', 'ἔτι', 'ἡ', 'ἢ', 'ἣ', 'ἤ', 'ἥ', 'ἧς', 'ἵνα', 'ὁ', 'ὃ', 'ὃν', 'ὃς', 'ὅ', 'ὅδε', 'ὅθεν', 'ὅπερ', 'ὅς', 'ὅσ', 'ὅστις', 'ὅστισ', 'ὅτε', 'ὅτι', 'ὑμόσ', 'ὑπ', 'ὑπέρ', 'ὑπό', 'ὑπὲρ', 'ὑπὸ', 'ὡς', 'ὡσ', 'ὥς', 'ὥστε', 'ὦ', 'ᾧ']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_english = stopwords.words('english')\n",
    "print('\\n Total English Stopwords: ', len(stop_english))\n",
    "print(stop_english)\n",
    "\n",
    "stop_german = stopwords.words('german')\n",
    "print('\\n Total German Stopwords: ', len(stop_german))\n",
    "print(stop_german)\n",
    "\n",
    "stop_greek = stopwords.words('greek')\n",
    "print('\\n Total Greek Stopwords: ', len(stop_greek))\n",
    "print(stop_greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', 'companion', 'sat', 'front', 'trap', 'arms', 'folded', 'hat', 'pulled', 'eyes', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought', 'suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "# Words apart from Stop words\n",
    "tokens = [token for token in lower_tokens if token not in stop_english]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'comput', 'comput', 'comput', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Stemming the text using porter stemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied', 'computer', 'computing', 'compute', 'better']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'deni', 'comput', 'comput', 'comput', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Stemming the text using Snowball stemmer\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','computer','computing','compute','better']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mul', 'deny', 'comput', 'comput', 'comput', 'bet']\n"
     ]
    }
   ],
   "source": [
    "# Stemming the text using Lancaster Stemmer\n",
    "lancaster = nltk.stem.LancasterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','computer','computing','compute','better']\n",
    "singles = [lancaster.stem(plural) for plural in plurals]\n",
    "print(singles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'miles', 'lovely', 'surrey', 'lanes', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'clouds', 'heavens', 'trees', 'wayside', 'hedges', 'throwing', 'first', 'green', 'shoots', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', 'companion', 'sat', 'front', 'trap', 'arms', 'folded', 'hat', 'pulled', 'eyes', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought', 'suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['waterloo', 'fortunate', 'catching', 'train', 'leatherhead', 'hired', 'trap', 'station', 'inn', 'drove', 'four', 'five', 'mile', 'lovely', 'surrey', 'lane', 'perfect', 'day', 'bright', 'sun', 'fleecy', 'cloud', 'heaven', 'tree', 'wayside', 'hedge', 'throwing', 'first', 'green', 'shoot', 'air', 'full', 'pleasant', 'smell', 'moist', 'earth', 'least', 'strange', 'contrast', 'sweet', 'promise', 'spring', 'sinister', 'quest', 'upon', 'engaged', 'companion', 'sat', 'front', 'trap', 'arm', 'folded', 'hat', 'pulled', 'eye', 'chin', 'sunk', 'upon', 'breast', 'buried', 'deepest', 'thought', 'suddenly', 'however', 'started', 'tapped', 'shoulder', 'pointed', 'meadow']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fly', 'dy', 'mule', 'denied', 'computer', 'computing', 'compute', 'better']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing the plurals example text\n",
    "lmtzr = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied','computer','computing','compute','better']\n",
    "singles = [lmtzr.lemmatize(token) for token in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation Removal\n",
    "Rmove punctuation, as it doesn’t add any extra information while treating text data. \n",
    "Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "['duchess', 'said', 'hopeful', 'tone', 'though', 'pepper', 'kitchen', 'soup', 'well', 'without', 'maybe', 'always', 'pepper', 'makes', 'people', 'hot', 'tempered']\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "['duchess', 'said', 'hopeful', 'tone', 'though', 'pepper', 'kitchen', 'soup', 'well', 'without', 'maybe', 'always', 'pepper', 'make', 'people', 'hot', 'tempered']\n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "['duchess', 'said', 'hope', 'tone', 'though', 'pepper', 'kitchen', 'soup', 'well', 'without', 'mayb', 'alway', 'pepper', 'make', 'peopl', 'hot', 'temper']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
    "\n",
    "singles = re.findall(r'\\w+', raw)\n",
    "print(singles)\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "##print(stops)\n",
    "\n",
    "tokens = [single.lower() for single in singles if single.lower() not in stops]\n",
    "print(tokens)\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "lmt = nltk.stem.WordNetLemmatizer()\n",
    "lmt_tokens = [lmt.lemmatize(token.lower()) for token in tokens]\n",
    "print(lmt_tokens)\n",
    "\n",
    "print('------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "lmt = nltk.stem.SnowballStemmer('english')\n",
    "lmt_tokens = [lmt.stem(token.lower()) for token in tokens]\n",
    "print(lmt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's combine all the above functions into a single function\n",
    "#### Take this up as an exercise and try to write your own function which takes in a string and does the following (in the same order)\n",
    "1. Sentenence Tokenize\n",
    "2. Word tokenize on each sentence\n",
    "3. Lower case\n",
    "4. Stop word removal\n",
    "5. Lemmatizing each word\n",
    "\n",
    "If the input string is \n",
    "```python\n",
    "'The quick brown fox. Jumped over the lazy dog.'\n",
    "```\n",
    "The output should be as follows\n",
    "```python\n",
    "[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n",
    "```\n",
    "Write your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox. Jumped over the lazy dog.\n",
      "[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "str = 'The quick brown fox. Jumped over the lazy dog.'\n",
    "print(str)\n",
    "\n",
    "sentence = sent_tokenize(str)\n",
    "#print(sentence)\n",
    "\n",
    "sent_token = [word_tokenize(sent) for sent in sentence]\n",
    "#print(sent_token)\n",
    "\n",
    "stopWords = stopwords.words('english')\n",
    "#print(stopWords)\n",
    "\n",
    "token = [[token.lower() for token in sent if token.lower() not in stopWords] for sent in sent_token]\n",
    "print(token)\n",
    "\n",
    "#tokens = [token for token in lower_tokens if token not in stop_english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def assignment(text):\n",
    "    sentence = nltk.tokenize.sent_tokenize(str)\n",
    "    \n",
    "    sent_token = [nltk.tokenize.word_tokenize(sent) for sent in sentence]\n",
    "    \n",
    "    stopWords = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    token = [[token.lower() for token in sent if token.lower() not in stopWords] for sent in sent_token]\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox. Jumped over the lazy dog.\n",
      "[['quick', 'brown', 'fox', '.'], ['jumped', 'lazy', 'dog', '.']]\n"
     ]
    }
   ],
   "source": [
    "string = 'The quick brown fox. Jumped over the lazy dog.'\n",
    "processed_tokens = assignment(string)\n",
    "print(string)\n",
    "print(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Solution\n",
    "\n",
    "    # Applying the necessary text preprocessing steps on the data\n",
    "def process_text(text):\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    sentence_tokens = [nltk.tokenize.word_tokenize(sentence) \n",
    "                       for sentence in sentences]\n",
    "#         tokens = []\n",
    "#         for sentence in sentence_tokens:\n",
    "#             sent = []\n",
    "#             for word in sentence:\n",
    "#                 if word.lower() not in stop:\n",
    "#                     sent.append(word.lower())\n",
    "#             tokens.append(sent)\n",
    "        ##THE SAME FOR LOOP CAN BE WRITTEN AS FOLLOWS\n",
    "    tokens = [[word.lower() for word in sent if word not in stop] \n",
    "              for sent in sentence_tokens]\n",
    "    tokens = [[lmtzr.lemmatize(word) for word in sent] for sent in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of the word \"upon\" during initialization is: 0\n"
     ]
    }
   ],
   "source": [
    "word_count = nltk.FreqDist() #WE INITIALIZE AN EMPTY FREQUENCY COUNTER\n",
    "print('Frequency of the word \"upon\" during initialization is: {}'.format(word_count['upon']))\n",
    "#Which means the default frequency is set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['duchess', 'said', 'hopeful', 'tone', 'though', 'pepper', 'kitchen', 'soup', 'well', 'without', 'maybe', 'always', 'pepper', 'makes', 'people', 'hot', 'tempered']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of duchess after update is: 1\n",
      "Frequency of said after update is: 1\n",
      "Frequency of hopeful after update is: 1\n",
      "Frequency of tone after update is: 1\n",
      "Frequency of though after update is: 1\n",
      "Frequency of pepper after update is: 1\n",
      "Frequency of kitchen after update is: 1\n",
      "Frequency of soup after update is: 1\n",
      "Frequency of well after update is: 1\n",
      "Frequency of without after update is: 1\n",
      "Frequency of maybe after update is: 1\n",
      "Frequency of always after update is: 1\n",
      "Frequency of pepper after update is: 2\n",
      "Frequency of makes after update is: 1\n",
      "Frequency of people after update is: 1\n",
      "Frequency of hot after update is: 1\n",
      "Frequency of tempered after update is: 1\n"
     ]
    }
   ],
   "source": [
    "# We update the frequency for each word as follows.\n",
    "for token in tokens:\n",
    "    word_count[token] += 1\n",
    "    print('Frequency of' , token, 'after update is: {}'.format(word_count[token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pepper', 2), ('duchess', 1), ('said', 1), ('hopeful', 1), ('tone', 1)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping wordcount into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('word_freqs.json', 'w') as f:\n",
    "    json.dump(word_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "<br>N-grams are the combination of multiple words used together. \n",
    "<br>Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.\n",
    "\n",
    "<br>Unigrams do not usually contain as much information as compared to bigrams and trigrams. \n",
    "<br>The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. \n",
    "<br>The longer the n-gram (the higher the n), the more context you have to work with. \n",
    "<br>Optimum length really depends on the application \n",
    "– if your n-grams are too short, you may fail to capture important differences. \n",
    "<br>On the other hand, if they are too long, you may fail to capture the “general knowledge” and only stick to particular cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('duchess', 'said')\n",
      "('said', 'hopeful')\n",
      "('hopeful', 'tone')\n",
      "('tone', 'though')\n",
      "('though', 'pepper')\n",
      "('pepper', 'kitchen')\n",
      "('kitchen', 'soup')\n",
      "('soup', 'well')\n",
      "('well', 'without')\n",
      "('without', 'maybe')\n",
      "('maybe', 'always')\n",
      "('always', 'pepper')\n",
      "('pepper', 'makes')\n",
      "('makes', 'people')\n",
      "('people', 'hot')\n",
      "('hot', 'tempered')\n"
     ]
    }
   ],
   "source": [
    "for each_bigram in nltk.ngrams(tokens, 2):\n",
    "    print (each_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('duchess', 'said', 'hopeful')\n",
      "('said', 'hopeful', 'tone')\n",
      "('hopeful', 'tone', 'though')\n",
      "('tone', 'though', 'pepper')\n",
      "('though', 'pepper', 'kitchen')\n",
      "('pepper', 'kitchen', 'soup')\n",
      "('kitchen', 'soup', 'well')\n",
      "('soup', 'well', 'without')\n",
      "('well', 'without', 'maybe')\n",
      "('without', 'maybe', 'always')\n",
      "('maybe', 'always', 'pepper')\n",
      "('always', 'pepper', 'makes')\n",
      "('pepper', 'makes', 'people')\n",
      "('makes', 'people', 'hot')\n",
      "('people', 'hot', 'tempered')\n"
     ]
    }
   ],
   "source": [
    "for each_bigram in nltk.ngrams(tokens, 3):\n",
    "    print (each_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "* How many words are there in the text? \n",
    "* How many sentences are there in the text?\n",
    "* How many unique words are there in the text?\n",
    "* What is the average number of characters of a word in the text?\n",
    "* Remove the stop words and store the tokens in a list.\n",
    "* What are the 10 most common words in the processed text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solutions to assignment 2\n",
    "\n",
    "```python\n",
    "'Use below text for assignment'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '''At Waterloo we were fortunate in catching a don't train for Leatherhead, where we hired a trap at the station inn and drove for four or five miles through the lovely Surrey lanes. \n",
    "It was a perfect day, with a bright sun and a few fleecy clouds in the heavens. \n",
    "The trees and wayside hedges were just throwing out their first green shoots, and the air was full of the pleasant smell of the moist earth. To me at least there was a strange contrast between the sweet promise of the spring and this sinister quest upon which we were engaged. \n",
    "My companion sat in the front of the trap, his arms folded, his hat pulled down over his eyes, and his chin sunk upon his breast, buried in the deepest thought. \n",
    "Suddenly, however, he started, tapped me on the shoulder, and pointed over the meadows.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at', 'waterloo', 'we', 'were', 'fortunate', 'in', 'catching', 'a', \"don't\", 'train', 'for', 'leatherhead', 'where', 'we', 'hired', 'a', 'trap', 'at', 'the', 'station', 'inn', 'and', 'drove', 'for', 'four', 'or', 'five', 'miles', 'through', 'the', 'lovely', 'surrey', 'lanes', 'it', 'was', 'a', 'perfect', 'day', 'with', 'a', 'bright', 'sun', 'and', 'a', 'few', 'fleecy', 'clouds', 'in', 'the', 'heavens', 'the', 'trees', 'and', 'wayside', 'hedges', 'were', 'just', 'throwing', 'out', 'their', 'first', 'green', 'shoots', 'and', 'the', 'air', 'was', 'full', 'of', 'the', 'pleasant', 'smell', 'of', 'the', 'moist', 'earth', 'to', 'me', 'at', 'least', 'there', 'was', 'a', 'strange', 'contrast', 'between', 'the', 'sweet', 'promise', 'of', 'the', 'spring', 'and', 'this', 'sinister', 'quest', 'upon', 'which', 'we', 'were', 'engaged', 'my', 'companion', 'sat', 'in', 'the', 'front', 'of', 'the', 'trap', 'his', 'arms', 'folded', 'his', 'hat', 'pulled', 'down', 'over', 'his', 'eyes', 'and', 'his', 'chin', 'sunk', 'upon', 'his', 'breast', 'buried', 'in', 'the', 'deepest', 'thought', 'suddenly', 'however', 'he', 'started', 'tapped', 'me', 'on', 'the', 'shoulder', 'and', 'pointed', 'over', 'the', 'meadows']\n"
     ]
    }
   ],
   "source": [
    "#from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(\"[\\w']+\")\n",
    "tokens = tokenizer.tokenize(string)\n",
    "\n",
    "tokens = [token.lower() for token in tokens]\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#1.\n",
    "print(len(tokens))\n",
    "\n",
    "#2. Sentences\n",
    "sentences = sent_tokenize(string)\n",
    "print(len(sentences))\n",
    "\n",
    "#3. unique words\n",
    "print(len(set(tokens)))\n",
    "\n",
    "#4. Average word length\n",
    "print (np.mean([len(token) for token in tokens]))\n",
    "\n",
    "#5. remove stopwords\n",
    "word_actual = [word for word in tokens if word not in stop]\n",
    "print(len(word_actual))\n",
    "print(word_actual)\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = nltk.FreqDist()\n",
    "for token in tokens:\n",
    "        word_count[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(word_count.keys()))\n",
    "print(word_count.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = list(word_count.keys())\n",
    "print(type(A[0]))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Generation of wordcloud\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "text = '''Sachin Ramesh Tendulkar; born 24 April 1973 is a former Indian international cricketer and a former captain of the Indian national team, regarded as one of the greatest batsmen of all time.[4] He is the highest run scorer of all time in International cricket. Often referred to as the 'God of Cricket' by Indian cricket followers,[5] Tendulkar took up cricket at the age of eleven, made his Test debut on 15 November 1989 against Pakistan in Karachi at the age of sixteen, and went on to represent Mumbai domestically and India internationally for close to twenty-four years. He is the only player to have scored one hundred international centuries, the first batsman to score a double century in a ODI, the holder of the record for the most number of runs in both Test and ODI, and the only player to complete more than 30,000 runs in international cricket.[6]\n",
    "\n",
    "In 2002, halfway through his career, Wisden Cricketers' Almanack ranked him the second greatest Test batsman of all time, behind Don Bradman, and the second greatest ODI batsman of all time, behind Viv Richards.[7] Later in his career, Tendulkar was a part of the Indian team that won the 2011 World Cup, his first win in six World Cup appearances for India.[8] He had previously been named \"Player of the Tournament\" at the 2003 edition of the tournament, held in South Africa. In 2013, he was the only Indian cricketer included in an all-time Test World XI named to mark the 150th anniversary of Wisden Cricketers' Almanack.[9][10][11]\n",
    "\n",
    "Tendulkar received the Arjuna Award in 1994 for his outstanding sporting achievement, the Rajiv Gandhi Khel Ratna award in 1997, India's highest sporting honour, and the Padma Shri and Padma Vibhushan awards in 1999 and 2008, respectively, India's fourth and second highest civilian awards.[12] After a few hours of his final match on 16 November 2013, the Prime Minister's Office announced the decision to award him the Bharat Ratna, India's highest civilian award.[13][14] He is the youngest recipient to date and the first ever sportsperson to receive the award.[15][16] He also won the 2010 Sir Garfield Sobers Trophy for cricketer of the year at the ICC awards.[17] In 2012, Tendulkar was nominated to the Rajya Sabha, the upper house of the Parliament of India.[18] He was also the first sportsperson and the first person without an aviation background to be awarded the honorary rank of group captain by the Indian Air Force.[19] In 2012, he was named an Honorary Member of the Order of Australia.[20][21]\n",
    "\n",
    "In 2010, Time magazine included Sachin in its annual Time 100 list as one of the \"Most Influential People in the World\".[22] In December 2012, Tendulkar announced his retirement from ODIs.[23] He retired from Twenty20 cricket in October 2013[24] and subsequently retired from all forms of cricket on 16 November 2013 after playing his 200th Test match, against the West Indies in Mumbai's Wankhede Stadium.[25] Tendulkar played 664 international cricket matches in total, scoring 34,357 runs.[6]'''\n",
    "\n",
    "A = word_tokenize(text)\n",
    "B = nltk.FreqDist(A)\n",
    "print(type(B))\n",
    "#print(B.items())\n",
    "print(B.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(B.keys())\n",
    "for i in B:\n",
    "    print (i +\" : \"+ str(B[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "cloud=WordCloud().generate(text)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cloud)\n",
    "plt.show(cloud)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
